# -*- coding: utf-8 -*-
"""BERT_Finetuning_mimic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kPyFusieFx5yq_6fMYy-qkvUCxspJhA4
"""

#importamos las librerias
import os
import pandas as pd
import re
import sys


#Vamos a instalar unas libreria, especificamente la libreria transformers de "huggingface"
#!pip3 install transformers sentencepiece

#importamos la libreria de transformers
import transformers
#y multiples clasificadores
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup

#Importamos la libreria para hacer el split train/test
from sklearn.model_selection import train_test_split

from sklearn.metrics import roc_curve
from sklearn.metrics import auc
from torch.nn.utils.rnn import pack_padded_sequence


# Importamos las librerias de pytorch
import torch
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler

numFold = sys.argv[1]
path_train = f'folds/fold{numFold}-train.csv'
path_test = f'folds/fold{numFold}-test.csv'

path_resultados = f'Resultados/completoBert-final/fold{numFold}-resultados.txt'
f_resultado = open(path_resultados, 'w')

train_df = pd.read_csv(f'fold-final/fold{numFold}-train.csv', nrows=50)
test_df = pd.read_csv(f'fold-final/fold{numFold}-test.csv', nrows=50)


lengths = train_df.Text.apply(lambda x: len(x.split(" ")))
lengths.max()


#model_name = 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12'
#model_name = 'emilyalsentzer/Bio_ClinicalBERT'
model_name = 'bert-base-uncased'
#Cargamos un modelo de bert y su tokenizador
tokenizer = BertTokenizer.from_pretrained(model_name, use_fast=False)
bert_model = BertModel.from_pretrained(model_name, return_dict=False)



#Vamos a generar un CustomDataset (una formar de cargar datos en batches)
#Para pytorch, nos va a facilitar el entrenamiento
class CustomDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.sentence = dataframe.Text.values
        self.targets = self.data.label.values
        self.max_len = max_len
    def __len__(self):
        return len(self.sentence)
    def __getitem__(self, index):
        sentence = str(self.sentence[index])
        sentence = " ".join(sentence.split())
        inputs = self.tokenizer.encode_plus(
            sentence,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            pad_to_max_length=True,
            return_token_type_ids=True
        )
        ids = inputs['input_ids'] #Ids del vocabulario
        mask = inputs['attention_mask'] #Mascaras para definir donde la atencion deberia ver
        token_type_ids = inputs["token_type_ids"] #requerido pero no utilizado
        
        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        }

#Generamos nuestra conexion a la GPU
from torch import cuda
device = 'cuda:0' if cuda.is_available() else 'cpu'
device = 'cpu'
print(device)

# Definimos constantes para la ejecucion de nuestro codigo
MAX_LEN = 512 # 2470
TRAIN_BATCH_SIZE = 8
TEST_BATCH_SIZE = 8
EPOCHS = 5
LEARNING_RATE = 1e-05

#Transformamos los df en datasets
training_set = CustomDataset(train_df, tokenizer, MAX_LEN)
val_set = CustomDataset(test_df, tokenizer, MAX_LEN)
test_set = CustomDataset(test_df, tokenizer, MAX_LEN)

train_params = {'batch_size': TRAIN_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }

test_params = {'batch_size': TEST_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }

#Luego generamos los dataloaders de pytorch

training_loader = DataLoader(training_set, **train_params)
testing_loader = DataLoader(test_set, **test_params)



from transformers.models.bert.modeling_bert import BertLayer
#Vamos a definir una clase de BERT modificada
class BERTClass(torch.nn.Module):
    def __init__(self,b_model):
        super(BERTClass, self).__init__()
        self.l1 = b_model # Definimos que la primera capa sea el modelo de bert que recibe por parametro
        self.hidden_size = b_model.config.hidden_size
        #self.l2 = torch.nn.LSTM(self.hidden_size, self.hidden_size, bidirectional=True)
        self.l2 = torch.nn.Dropout(0.1) # A la salida de bert, vamos a aplicar un dropout de 0.1
        self.l3 = torch.nn.Linear(self.hidden_size*2, 1)#Finalmente vamos a definir una capa densa (combinaci칩n lineal) de la dimension de bert a nuestra neurona de salida
        #self.out_activation = torch.nn.Sigmoid() # funci칩n de activaci칩n (que no vamos a utilizar)
        ##self.out_activation = torch.nn.LSTM(500, 1)


    def forward(self, ids, mask, token_type_ids):
        #Hacemos el forward pass por BERT, dropout y la capa densa
        
        #encoded_layers, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids) 
        #encoded_layers = encoded_layers.permute(1,0,2)
        #enc_hidden, (last_hidden, last_cell) = self.l2(pack_padded_sequence(encoded_layers, token_type_ids))
        #output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)
        #output_hidden = torch.nn.functional.dropout(output_hidden, 0.1)
        #output = self.l3(output_hidden)
        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)
        output_2 = self.l2(output_1)
        output = self.l3(output_2)
        output = self.out_activation(output)
        return output

#Generamos un modelo de BERTClass
model = BERTClass(bert_model)
#llevamos el modelo a la GPU
model.to(device)

#Definimos nuestra funcion de perdida
def loss_fn(outputs, targets):
    #return torch.nn.CrossEntropyLoss()(outputs, targets)
    return torch.nn.BCEWithLogitsLoss()(outputs, targets)

#El optimizador
optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)

from tqdm.notebook import tqdm

#Definimos nuestro loop de entrenamiento
def train(epoch):
    
    model.train()
    loss_acum=0
    for iters , data in tqdm(enumerate(training_loader, 0), total = len(training_loader)):
        #print(len(data['ids'][0]))
        #print(len(data['mask'][0]))
        #print(len(data['token_type_ids'][0]))
        ids = data['ids'].to(device, dtype = torch.long)
        mask = data['mask'].to(device, dtype = torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.float)
        #print(len(ids),len(mask),len(token_type_ids))
        #print(ids,mask,token_type_ids,targets)
        outputs = model(ids, mask, token_type_ids)
        outputs = outputs.squeeze(1)
        #print(outputs,targets)
        optimizer.zero_grad()
        loss = loss_fn(outputs, targets)
        loss_acum += loss.item()
        if iters%5000==0:
            print(outputs,targets)
            print(f'Epoch: {epoch}, Loss:  {loss.item()}')
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(iters)
    return loss_acum/iters

#Definimos nuestro loop de validacion/testing
def validation(epoch):
    model.eval()
    fin_targets=[]
    fin_outputs=[]
    with torch.no_grad():
        for _, data in tqdm(enumerate(testing_loader, 0),total = len(testing_loader)):
            ids = data['ids'].to(device, dtype = torch.long)
            mask = data['mask'].to(device, dtype = torch.long)
            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)

            targets = data['targets'].to(device, dtype = torch.float)
            outputs = model(ids, mask, token_type_ids)
            fin_targets.extend(targets.cpu().detach().numpy().tolist())
            fin_outputs.extend(torch.sigmoid(outputs).squeeze(1).cpu().detach().numpy())
    
    return np.array(fin_outputs), np.array(fin_targets)


def generar_archivo_resultado(f, predicted, y_test):
    acc = metrics.accuracy_score(y_test, predicted)
    f_score = metrics.f1_score(y_test, predicted, average='weighted')
    recall = metrics.recall_score(y_test, predicted)
    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, predicted)
    auc_keras = auc(fpr_keras, tpr_keras)
    f.write(f'\tf_score: {f_score}\n')
    f.write(f'\trecall: {recall}\n')
    f.write(f'\taccuracy: {acc}\n')
    f.write(f'\tauroc: {auc_keras}\n')



#iteramos por nuestras epocas
for epoch in range(EPOCHS):
    epoch_loss = train(epoch)
    print(f'epoch loss: {epoch_loss}')

from sklearn import metrics
import numpy as np

#Corremos un loop de testing
for epoch in range(1):
    outputs, targets = validation(epoch)

#Calculamos los valores binarios y luego obtenemos las metricas de desempe침o
outputs_bin = (outputs >= 0.5)
#accuracy = metrics.accuracy_score(targets, outputs_bin)
#f1_score_micro = metrics.f1_score(targets, outputs_bin)
#recall = metrics.recall_score(targets, outputs_bin)


generar_archivo_resultado(f_resultado, outputs_bin, targets)



print(metrics.classification_report(targets, outputs_bin))

test_string = "i hated the movie, it did not have very beautiful scenes"

converted_string = tokenizer.encode_plus(test_string, return_tensors='pt')

converted_string

inputs_ids = (converted_string['input_ids']).to(device, dtype = torch.long)
attention_mask = (converted_string['attention_mask']).to(device, dtype = torch.long)
token_type_ids = (converted_string['token_type_ids']).to(device, dtype = torch.long)

outputs = model(inputs_ids,attention_mask, token_type_ids)
torch.sigmoid(outputs).squeeze(1).cpu().detach().numpy()

